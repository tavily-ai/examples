{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVdtcEDEW9hU"
   },
   "source": [
    "# Notebook App for Agentic Research Assistant Agent Using Tavily, LangGrpah and Cohere Command R+ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_cohere langchain-core langgraph langchain_core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tavily-python in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (0.3.4)\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from tavily-python) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: httpx in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from tavily-python) (0.27.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.5.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from requests->tavily-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from requests->tavily-python) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from requests->tavily-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from requests->tavily-python) (2024.6.2)\n",
      "Requirement already satisfied: anyio in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from httpx->tavily-python) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Downloading tavily_python-0.4.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: tavily-python\n",
      "  Attempting uninstall: tavily-python\n",
      "    Found existing installation: tavily-python 0.3.4\n",
      "    Uninstalling tavily-python-0.3.4:\n",
      "      Successfully uninstalled tavily-python-0.3.4\n",
      "Successfully installed tavily-python-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maitarasher/anaconda3/envs/tavily/lib/python3.12/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_cohere.chat_models import ChatCohere\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from tavily import AsyncTavilyClient, TavilyClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Your API Keys\n",
    "TAVILY_API_KEY = \"YOUR TAIVLY API KEY\"\n",
    "COHERE_API_KEY = \"YOUR COHERE API KEY\"\n",
    "OPENAI_API_KEY =  \"YOUR OPEN API KEY\"\n",
    "\n",
    "# Or use .env file \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import operator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Dict, Union\n",
    "import json\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    # Declare a dictionary where:\n",
    "    # - The outer dictionary has string keys.\n",
    "    # - The inner dictionary can have keys of different types (e.g., str, int).\n",
    "    # - The inner dictionary values can be of different types (e.g., str, float).\n",
    "    documents: Dict[str, Dict[Union[str, int], Union[str, float]]]\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    source_id: str = Field(\n",
    "        ...,\n",
    "        description=\"The url of a SPECIFIC source which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        ...,\n",
    "        description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources. Include any relevant sources in the answer as markdown hyperlinks. For example: 'This is a sample text ([url website](url))'\"\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        ..., description=\"Citations from the given sources that justify the answer.\"\n",
    "    )\n",
    "class Query(BaseModel):\n",
    "    query: str = Field(description=\"\n",
    "# Define args_schema for tavily search\n",
    "class TavilySearchInput(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"break down the user's input into a set of sub-queries / sub-problems that can be answered in isolation\")\n",
    "    topic: str = Field(description=\"type of search, should be 'general' or 'news'\")\n",
    "    days: int = Field(description=\"number of days back to run 'news' search\")\n",
    "\n",
    "@tool(\"tavily_search\",args_schema=TavilySearchInput, return_direct=True)\n",
    "async def tavily_search(sub_queries: List[str], topic: str, days: int):\n",
    "    \"\"\"Perform searches for each sub-query using the Tavily search tool.\"\"\"\n",
    "    search_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        response = await tavily_client.search(query=sub_query, topic=topic, days=days,include_raw_content=False)\n",
    "        search_results.extend(response['results'])\n",
    "    return search_results\n",
    "\n",
    "\n",
    "tools = [tavily_search]\n",
    "tavily_client = AsyncTavilyClient()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0).bind_tools(tools)\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "async def tool_node(state: ResearchState):\n",
    "    docs = state['documents'] or {}\n",
    "    docs_str = \"\"\n",
    "    msgs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        new_docs = await tool.ainvoke(tool_call[\"args\"])\n",
    "        for doc in new_docs:\n",
    "            # Make sure that this document was not retrieved before\n",
    "            if not docs or doc['url'] not in docs:\n",
    "                docs[doc['url']] = doc\n",
    "                docs_str += json.dumps(doc)\n",
    "        msgs.append(ToolMessage(content=f\"Found the following new documents: {docs_str}\", tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": msgs, \"documents\": docs}\n",
    "    \n",
    "        \n",
    "def call_model(state: ResearchState):\n",
    "    messages = state['messages']\n",
    "    # print(\"state['messages']:\",state['messages'])\n",
    "    response = model.invoke(messages)\n",
    "    print(response)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_model_with_docs(state: ResearchState):\n",
    "    messages = state['messages']\n",
    "    response = model.with_structured_output(QuotedAnswer).invoke(input=messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [AIMessage(content=response.answer)]}\n",
    "    \n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: ResearchState) -> Literal[\"tools\", \"RAG model\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return \"RAG model\"\n",
    "\n",
    "# Define a graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"route_query\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"RAG model\", call_model_with_docs)\n",
    "# Set the entrypoint as route_query\n",
    "workflow.set_entry_point(\"route_query\")\n",
    "\n",
    "# Determine which node is called next\n",
    "workflow.add_conditional_edges(\n",
    "    \"route_query\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `tools` to `route_query`.\n",
    "# This means that after `tools` is called, `route_query` node is called next.\n",
    "workflow.add_edge(\"tools\", \"route_query\")\n",
    "workflow.add_edge(\"RAG model\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Wild fire prevention startups, divided by the type of technology\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_ONW5tlNDpKYuQMiigukZq2Xc', 'function': {'arguments': '{\"sub_queries\":[\"wildfire prevention startups\",\"wildfire prevention technology types\",\"wildfire prevention startups by technology\"],\"topic\":\"general\",\"days\":30}', 'name': 'tavily_search'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 120, 'total_tokens': 162}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f4110bd2-c393-482f-a516-08821ab65f14-0' tool_calls=[{'name': 'tavily_search', 'args': {'sub_queries': ['wildfire prevention startups', 'wildfire prevention technology types', 'wildfire prevention startups by technology'], 'topic': 'general', 'days': 30}, 'id': 'call_ONW5tlNDpKYuQMiigukZq2Xc', 'type': 'tool_call'}] usage_metadata={'input_tokens': 120, 'output_tokens': 42, 'total_tokens': 162}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_ONW5tlNDpKYuQMiigukZq2Xc)\n",
      " Call ID: call_ONW5tlNDpKYuQMiigukZq2Xc\n",
      "  Args:\n",
      "    sub_queries: ['wildfire prevention startups', 'wildfire prevention technology types', 'wildfire prevention startups by technology']\n",
      "    topic: general\n",
      "    days: 30\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Found the following new document: {\"title\": \"As California wildfire season nears, startup BurnBot is working on a ...\", \"url\": \"https://www.cnbc.com/2024/04/02/burnbot-raises-20-million-to-build-technology-for-wildfire-prevention.html\", \"content\": \"BurnBot raised $20 million for technology and services to prevent wildfires. The startup was founded in 2022 and makes remote controlled vehicles that can munch up and burn away invasive plants ...\", \"score\": 0.9958175, \"raw_content\": null}{\"title\": \"Top 11 Innovative Startups Applying AI for Wildfires Detection ...\", \"url\": \"https://www.omdena.com/blog/top-ai-wildfires-detection-startups\", \"content\": \"With global warming, the number of wildfires is expected to increase. Artificial intelligence can help prevent wildfires in combination with technologies such as drones or infrared cameras. For example, startups like Dryad develop an IoT network that enables public and private forest owners to monitor, analyze, and protect the world's largest ...\", \"score\": 0.99097973, \"raw_content\": null}{\"title\": \"These Tech Companies Think They Can 'Solve' the Wildfire Crisis\", \"url\": \"https://www.vice.com/en/article/these-tech-companies-think-they-can-solve-the-wildfire-crisis/\", \"content\": \"Tim Barat, the co-founder of Gridware, a utility monitoring startup, says the first slide of his pitch dek to investors is a mirror. ... He saw a market finally ready to take wildfire prevention ...\", \"score\": 0.99097973, \"raw_content\": null}{\"title\": \"Wildfire startup puts AI-powered eyes in the forest to watch for new ...\", \"url\": \"https://www.geekwire.com/2023/wildfire-startup-puts-ai-powered-eyes-in-the-forest-to-watch-for-new-blazes-and-provide-rapid-alerts/\", \"content\": \"Now there's hope that Pano AI and other technologies flooding the wildfire realm can help prevent and limit wildfire damage. DNR is in discussion with established corporations and startups to ...\", \"score\": 0.98840266, \"raw_content\": null}{\"title\": \"Colorado wildfires: How giant balloons could help to predict future ...\", \"url\": \"https://www.cnn.com/weather/colorado-wildfires-urban-sky-balloon-c2e-spc/index.html\", \"content\": \"US startup Urban Sky is launching balloons into the stratosphere to test the technology as an inexpensive way to detect, track, and ultimately prevent the spread of wildfires.\", \"score\": 0.986828, \"raw_content\": null}{\"title\": \"Advancements in Forest Fire Prevention: A Comprehensive Survey | MDPI\", \"url\": \"https://www.mdpi.com/1424-8220/23/14/6635\", \"content\": \"Understanding the evolution of technology in addressing this issue is essential to formulate more effective strategies for mitigating and preventing wildfires. ... sensor types, and data processing techniques. By incorporating different type of sensors, wireless nodes have the capability to detect a range of physical parameters including ...\", \"score\": 0.97719735, \"raw_content\": null}{\"title\": \"The Role Of Technology In Wildfire Prevention And Response | Forbes\", \"url\": \"https://www.forbes.com/councils/forbestechcouncil/2024/02/23/the-role-of-technology-in-wildfire-prevention-and-response/\", \"content\": \"1. Preventing Wildfires From Starting. New, technology-assisted approaches to fire prevention are making headway. Drones, for example, enable operators to rapidly and safely assess on-the-ground ...\", \"score\": 0.9628831, \"raw_content\": null}{\"title\": \"Advancing Technology to Support Wildland Fire Mitigation and Response\", \"url\": \"https://www.doi.gov/wildlandfire/advancing-technology-support-wildland-fire-mitigation-and-response\", \"content\": \"The Wildland Fire Information and Technology Program is advancing the development and deployment of the tools wildfire personnel depend upon. From hand-held devices to satellites, technology is essential to effectively respond to and mitigate the threat of wildland fires. The Wildland Fire Information and Technology Program is advancing the ...\", \"score\": 0.8726733, \"raw_content\": null}{\"title\": \"New technologies that could spark change in wildfire risk reduction ...\", \"url\": \"https://www.preventionweb.net/news/8-blazing-new-technologies-could-spark-change-wildfire-risk-reduction\", \"content\": \"2. Smart risk maps give 80% accuracy in T\\u00fcrkiye. An interactive wildfire risk map - developed using AI and machine learning drawing on historical, meteorological, and geographical data - achieves an 80% accuracy rate in predicting wildfires 24 hours before they occur, giving authorities crucial time to prepare and respond proactively.\", \"score\": 0.6522414, \"raw_content\": null}{\"title\": \"Technology to Reduce the Impacts of Wildfires | Homeland Security\", \"url\": \"https://www.dhs.gov/science-and-technology/technology-reduce-impacts-wildfires\", \"content\": \"Wildfire Sensors. Wildfires across the U.S. and around the world are becoming more frequent, costly, and dangerous. Early detection of ignition increases the likelihood of timely containment and suppression of wildfires, saving lives and reducing property losses. Wildfire sensors research focuses upon real-time and continuous identification of ...\", \"score\": 0.33372033, \"raw_content\": null}{\"title\": \"AI is helping in the fight against wildfires - Los Angeles Times\", \"url\": \"https://www.latimes.com/business/story/2023-09-27/ai-robots-satellite-sensors-wildfires\", \"content\": \"San Francisco-based Pano AI, which has built about 100 AI-enabled fire lookouts in six U.S. states and Australia, is one of a growing number of startups leveraging technology to aid in wildfire ...\", \"score\": 0.9916842, \"raw_content\": null}{\"title\": \"Advanced Wildfire + Bushfire Detection Technology | Pano AI\", \"url\": \"https://www.pano.ai/\", \"content\": \"Pano uses deep learning AI and computer vision to automatically detect, verify and classify wildfire events in real time. Pano software enables real-time viewing and triage of wildfire alerts, and built-in communication tools to get information out to the field. Rapid confirmation of potential fires matters just as much as initial detection.\", \"score\": 0.7183969, \"raw_content\": null}\n",
      "content='Here are some notable startups focused on wildfire prevention, categorized by the type of technology they utilize:\\n\\n### 1. **AI and Machine Learning**\\n- **Pano AI**: This startup has developed AI-enabled fire lookouts that use deep learning and computer vision to detect and classify wildfire events in real-time. [More Info](https://www.latimes.com/business/story/2023-09-27/ai-robots-satellite-sensors-wildfires)\\n- **Dryad**: They create an IoT network for monitoring and protecting forests, utilizing AI to analyze data and prevent wildfires. [More Info](https://www.omdena.com/blog/top-ai-wildfires-detection-startups)\\n\\n### 2. **Drones and Remote Sensing**\\n- **BurnBot**: This startup focuses on remote-controlled vehicles that can clear invasive plants, which helps in reducing wildfire risks. [More Info](https://www.cnbc.com/2024/04/02/burnbot-raises-20-million-to-build-technology-for-wildfire-prevention.html)\\n- **Urban Sky**: They are testing balloon technology to detect and track wildfires from the stratosphere. [More Info](https://www.cnn.com/weather/colorado-wildfires-urban-sky-balloon-c2e-spc/index.html)\\n\\n### 3. **Sensor Technologies**\\n- **Gridware**: This utility monitoring startup uses advanced sensors to detect potential wildfire risks and monitor environmental conditions. [More Info](https://www.vice.com/en/article/these-tech-companies-think-they-can-solve-the-wildfire-crisis/)\\n- **Wildfire Sensors**: Research is ongoing to develop sensors that provide real-time identification of wildfires, enhancing early detection and response capabilities. [More Info](https://www.dhs.gov/science-and-technology/technology-reduce-impacts-wildfires)\\n\\n### 4. **Predictive Analytics**\\n- **Smart Risk Maps**: Utilizing AI and machine learning, these maps predict wildfire occurrences with high accuracy, allowing for proactive measures. [More Info](https://www.preventionweb.net/news/8-blazing-new-technologies-could-spark-change-wildfire-risk-reduction)\\n\\nThese startups represent a diverse range of technologies aimed at preventing and mitigating the impact of wildfires, from AI and drones to advanced sensor systems and predictive analytics.' response_metadata={'token_usage': {'completion_tokens': 488, 'prompt_tokens': 1591, 'total_tokens': 2079}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-8e96200d-051a-44a6-bc10-405ca1bd5028-0' usage_metadata={'input_tokens': 1591, 'output_tokens': 488, 'total_tokens': 2079}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are some notable startups focused on wildfire prevention, categorized by the type of technology they utilize:\n",
      "\n",
      "### 1. **AI and Machine Learning**\n",
      "- **Pano AI**: This startup has developed AI-enabled fire lookouts that use deep learning and computer vision to detect and classify wildfire events in real-time. [More Info](https://www.latimes.com/business/story/2023-09-27/ai-robots-satellite-sensors-wildfires)\n",
      "- **Dryad**: They create an IoT network for monitoring and protecting forests, utilizing AI to analyze data and prevent wildfires. [More Info](https://www.omdena.com/blog/top-ai-wildfires-detection-startups)\n",
      "\n",
      "### 2. **Drones and Remote Sensing**\n",
      "- **BurnBot**: This startup focuses on remote-controlled vehicles that can clear invasive plants, which helps in reducing wildfire risks. [More Info](https://www.cnbc.com/2024/04/02/burnbot-raises-20-million-to-build-technology-for-wildfire-prevention.html)\n",
      "- **Urban Sky**: They are testing balloon technology to detect and track wildfires from the stratosphere. [More Info](https://www.cnn.com/weather/colorado-wildfires-urban-sky-balloon-c2e-spc/index.html)\n",
      "\n",
      "### 3. **Sensor Technologies**\n",
      "- **Gridware**: This utility monitoring startup uses advanced sensors to detect potential wildfire risks and monitor environmental conditions. [More Info](https://www.vice.com/en/article/these-tech-companies-think-they-can-solve-the-wildfire-crisis/)\n",
      "- **Wildfire Sensors**: Research is ongoing to develop sensors that provide real-time identification of wildfires, enhancing early detection and response capabilities. [More Info](https://www.dhs.gov/science-and-technology/technology-reduce-impacts-wildfires)\n",
      "\n",
      "### 4. **Predictive Analytics**\n",
      "- **Smart Risk Maps**: Utilizing AI and machine learning, these maps predict wildfire occurrences with high accuracy, allowing for proactive measures. [More Info](https://www.preventionweb.net/news/8-blazing-new-technologies-could-spark-change-wildfire-risk-reduction)\n",
      "\n",
      "These startups represent a diverse range of technologies aimed at preventing and mitigating the impact of wildfires, from AI and drones to advanced sensor systems and predictive analytics.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are some notable startups focused on wildfire prevention, categorized by the type of technology they utilize:\n",
      "\n",
      "### 1. **AI and Machine Learning**\n",
      "- **Pano AI**: This startup has developed AI-enabled fire lookouts that use deep learning and computer vision to detect and classify wildfire events in real-time. [More Info](https://www.latimes.com/business/story/2023-09-27/ai-robots-satellite-sensors-wildfires)\n",
      "- **Dryad**: They create an IoT network for monitoring and protecting forests, utilizing AI to analyze data and prevent wildfires. [More Info](https://www.omdena.com/blog/top-ai-wildfires-detection-startups)\n",
      "\n",
      "### 2. **Drones and Remote Sensing**\n",
      "- **BurnBot**: This startup focuses on remote-controlled vehicles that can clear invasive plants, which helps in reducing wildfire risks. [More Info](https://www.cnbc.com/2024/04/02/burnbot-raises-20-million-to-build-technology-for-wildfire-prevention.html)\n",
      "- **Urban Sky**: They are testing balloon technology to detect and track wildfires from the stratosphere. [More Info](https://www.cnn.com/weather/colorado-wildfires-urban-sky-balloon-c2e-spc/index.html)\n",
      "\n",
      "### 3. **Sensor Technologies**\n",
      "- **Gridware**: This utility monitoring startup uses advanced sensors to detect potential wildfire risks and monitor environmental conditions. [More Info](https://www.vice.com/en/article/these-tech-companies-think-they-can-solve-the-wildfire-crisis/)\n",
      "- **Wildfire Sensors**: Research is ongoing to develop sensors that provide real-time identification of wildfires, enhancing early detection and response capabilities. [More Info](https://www.dhs.gov/science-and-technology/technology-reduce-impacts-wildfires)\n",
      "\n",
      "### 4. **Predictive Analytics**\n",
      "- **Smart Risk Maps**: Utilizing AI and machine learning, these maps predict wildfire occurrences with high accuracy, allowing for proactive measures. [More Info](https://www.preventionweb.net/news/8-blazing-new-technologies-could-spark-change-wildfire-risk-reduction)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Wild fire prevention startups, divided by the type of technology\"\n",
    "    )\n",
    "]\n",
    "\n",
    "async for s in app.astream({\"messages\": messages}, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.tools import tool\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# import operator\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from typing import Dict, Union\n",
    "# import json\n",
    "\n",
    "\n",
    "# class ResearchState(TypedDict):\n",
    "#     user_query: str\n",
    "#     critique: str\n",
    "#     answer: str\n",
    "#     documents: Annotated[list[dict], operator.add]\n",
    "#     #documents: List[dict]\n",
    "#     #documents: list[dict]\n",
    "#     web_queries: List[str]\n",
    "#     revision_number: int\n",
    "#     max_revisions: int\n",
    "#     messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# class Citation(BaseModel):\n",
    "#     source_id: int = Field(\n",
    "#         ...,\n",
    "#         description=\"The integer ID of a SPECIFIC source which justifies the answer.\",\n",
    "#     )\n",
    "#     quote: str = Field(\n",
    "#         ...,\n",
    "#         description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# class QuotedAnswer(BaseModel):\n",
    "#     \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "\n",
    "#     answer: str = Field(\n",
    "#         ...,\n",
    "#         description=\"The answer to the user question, which is based only on the given sources.\",\n",
    "#     )\n",
    "#     citations: List[Citation] = Field(\n",
    "#         ..., description=\"Citations from the given sources that justify the answer.\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # @tool(\"tavily_search\",args_schema=SearchInput, return_direct=True)\n",
    "# # async def tavily_search(query: str, topic: str):\n",
    "# #     \"\"\"Perform web search using the Tavily search tool.\"\"\"\n",
    "# #     return await tavily_client.search(query=query, topic=topic)\n",
    "\n",
    "# # Define args_schema for tavily search\n",
    "# class TavilySearchInput(BaseModel):\n",
    "#     sub_queries: List[str] = Field(description=\"break down the user's input into a set of sub-queries / sub-problems that can be answered in isolation\")\n",
    "#     topic: str = Field(description=\"type of search, should be 'general' or 'news'\")\n",
    "#     days: int = Field(description=\"number of days back to run 'news' search\")\n",
    "\n",
    "# @tool(\"tavily_search\",args_schema=TavilySearchInput, return_direct=True)\n",
    "# async def tavily_search(sub_queries: List[str], topic: str, days: int):\n",
    "#     \"\"\"Perform searches for each sub-query using the Tavily search tool.\"\"\"\n",
    "#     search_results = []\n",
    "#     for sub_query in sub_queries:\n",
    "#         response = await tavily_client.search(query=sub_query, topic=topic,include_raw_content=False)\n",
    "#         for r in response['results']:\n",
    "#             r.pop('raw_content', None)\n",
    "#             r['score'] = str(r['score']) # Converting to string for cohere\n",
    "#             search_results.append(r)\n",
    "#         # print(results)\n",
    "#         #search_results.extend(response['results'])\n",
    "#     # print(\"search_results\",search_results)\n",
    "#     return search_results\n",
    "\n",
    "\n",
    "# tools = [tavily_search]\n",
    "# # tool_node = ToolNode(tools)\n",
    "\n",
    "# tavily_client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0).bind_tools(tools)\n",
    "# # model_with_tools = ChatCohere(model=\"command-r-plus\", temperature=0).bind_tools(tools)\n",
    "\n",
    "# tools_by_name = {tool.name: tool for tool in tools}\n",
    "# async def tool_node(state: ResearchState):\n",
    "#     docs = []\n",
    "#     msgs = []\n",
    "#     for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         # print(tool)\n",
    "#         observation = await tool.ainvoke(tool_call[\"args\"])\n",
    "#         # print(observation)\n",
    "#         docs.extend(observation)\n",
    "#         msgs.append(ToolMessage(content=f\"Added documents: {observation}\", tool_call_id=tool_call[\"id\"]))\n",
    "#     # print(\"inside tool:\",docs)\n",
    "#     return {\"messages\": msgs, \"documents\": docs}\n",
    "    \n",
    "        \n",
    "# def call_model(state: ResearchState):\n",
    "#     messages = state['messages']\n",
    "#     # print(\"state['messages']:\",state['messages'])\n",
    "#     print(\"state['documents']:\",state['documents'])\n",
    "#     response = model.invoke(messages)\n",
    "#     print(response)\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# def call_model_with_docs(state: ResearchState):\n",
    "#     messages = state['messages']\n",
    "#     print(\"state['messages']:\",state['messages'])\n",
    "#     # print(\"state['documents']:\",state['documents'])\n",
    "#     response = model.with_structured_output(QuotedAnswer).invoke(input=messages)\n",
    "#     print(\"response with docs:\\n\",response)\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": [response]}\n",
    "# #COHERE\n",
    "# # def call_model_with_docs(state: ResearchState):\n",
    "# #     messages = state['messages']\n",
    "# #     # print(\"state['messages']:\",state['messages'])\n",
    "# #     print(\"state['documents']:\",state['documents'])\n",
    "# #     response = model.with_structured_output(QuotedAnswer).invoke(input=messages, \n",
    "# #                             preamble=\"\"\"You are an expert write a coherent and deatiled response based on the user's question with the most relevant datasources.\"\"\",\n",
    "# #                             documents=state['documents'])\n",
    "# #     # We return a list, because this will get added to the existing list\n",
    "# #     return {\"messages\": [response]}\n",
    "    \n",
    "# # Define the function that determines whether to continue or not\n",
    "# def should_continue(state: ResearchState) -> Literal[\"tools\", \"RAG model\"]:\n",
    "#     messages = state['messages']\n",
    "#     last_message = messages[-1]\n",
    "#     # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "#     if last_message.tool_calls:\n",
    "#         return \"tools\"\n",
    "#     # Otherwise, we stop (reply to the user)\n",
    "#     return \"RAG model\"\n",
    "\n",
    "# # Define a graph\n",
    "# workflow = StateGraph(ResearchState)\n",
    "\n",
    "# # Add nodes\n",
    "# workflow.add_node(\"route_query\", call_model)\n",
    "# workflow.add_node(\"tools\", tool_node)\n",
    "# workflow.add_node(\"RAG model\", call_model_with_docs)\n",
    "# # Set the entrypoint as route_query\n",
    "# workflow.set_entry_point(\"route_query\")\n",
    "\n",
    "# # Determine which node is called next\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"route_query\",\n",
    "#     # Next, we pass in the function that will determine which node is called next.\n",
    "#     should_continue,\n",
    "# )\n",
    "\n",
    "# # Add a normal edge from `tools` to `route_query`.\n",
    "# # This means that after `tools` is called, `route_query` node is called next.\n",
    "# workflow.add_edge(\"tools\", \"route_query\")\n",
    "# workflow.add_edge(\"RAG model\", END)\n",
    "\n",
    "# app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # tool descriptions that the model has access to\n",
    "        # tools = [\n",
    "        #    {\n",
    "        #        \"name\": \"tavily_search\",\n",
    "        #        \"description\": \"Connect to a general/news web search engine to gather more information on user's query\",\n",
    "        #        \"parameter_definitions\": {\n",
    "        #            \"type\": {\n",
    "        #                \"description\": \"type of search to run, 'general', 'news' or both\",\n",
    "        #                \"type\": \"str\",\n",
    "        #                \"required\": True\n",
    "        #            }\n",
    "        #        }\n",
    "        #    }\n",
    "        # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "colab": {
   "collapsed_sections": [
    "v1a8tvFOW9hX",
    "XUJuVVjV70y9",
    "2SBektD48VjA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
